{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5da2cf",
   "metadata": {},
   "source": [
    "### App 1\n",
    "The application allows users to enter a research topic, which triggers an agent to search arXiv, retrieve relevant papers, extract their content, convert them to embeddings, and store them in a vector database. Once indexing is complete, the system informs the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e1de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !capture --no-stderr\n",
    "# !pip install --quiet -U langchain_openai langchain_core langchain langchain-community langchain_pinecone beautifulsoup4 requests pinecone tabulate\n",
    "# !pip install arxiv\n",
    "# !pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98a3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv \n",
    "import pinecone \n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI \n",
    "import langchain \n",
    "import arxiv \n",
    "import requests \n",
    "from PyPDF2 import PdfReader \n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891c8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"arxiv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5995f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index: arxiv <pinecone.db_data.index.Index object at 0x00000270180C5160>\n"
     ]
    }
   ],
   "source": [
    "# pip install pinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"arxiv\")\n",
    "print(\"Connected to Pinecone index:\", PINECONE_INDEX_NAME, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54685d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4.1\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe9c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Embeddings\n",
    "embeddings_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    "    dimensions=1536\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c86cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install arxiv\n",
    "# import arxiv\n",
    "\n",
    "# def fetch_arxiv_papers(query, max_results=5):\n",
    "#     search = arxiv.Search(\n",
    "#         query=query,\n",
    "#         max_results=max_results,\n",
    "#         sort_by=arxiv.SortCriterion.Relevance\n",
    "#     )\n",
    "#     papers = []\n",
    "#     for result in search.results():\n",
    "#         pdf_url = result.pdf_url \n",
    "#         response = requests.get(pdf_url)\n",
    "#         pdf_file = BytesIO(response.content)\n",
    "#         reader = PdfReader(pdf_file)\n",
    "#         full_text = \"\"\n",
    "#         for page in reader.pages:\n",
    "#             full_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "#         papers.append({\n",
    "#             \"title\": result.title,\n",
    "#             \"abstract\": result.summary,\n",
    "#             \"url\": result.entry_id,\n",
    "#             \"pdf_text\": full_text\n",
    "#         })\n",
    "#     return papers\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=5):\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    client = arxiv.Client()  # new API client\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    for result in client.results(search):\n",
    "        try:\n",
    "            pdf_url = result.pdf_url\n",
    "            response = requests.get(pdf_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            pdf_file = BytesIO(response.content)\n",
    "            reader = PdfReader(pdf_file)\n",
    "            full_text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text + \"\\n\"\n",
    "\n",
    "            combined_text = (result.summary or \"\") + \"\\n\\n\" + full_text\n",
    "\n",
    "            papers.append({\n",
    "                \"id\": result.entry_id,\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"url\": result.entry_id,\n",
    "                \"pdf_text\": full_text,\n",
    "                \"text\": combined_text[:2000],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {result.title[:50]}... due to error: {e}\")\n",
    "\n",
    "    print(f\"✅ Retrieved {len(papers)} papers for query '{query}'\")\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def index_papers(papers):\n",
    "#     for paper in papers:\n",
    "#         vector = embeddings_model.embed_query(paper['pdf_text'])\n",
    "#         index.upsert([\n",
    "#             (\n",
    "#                 paper['url'],\n",
    "#                 vector,\n",
    "#                 {\n",
    "#                     \"title\": paper['title'],\n",
    "#                     \"url\": paper['url'],\n",
    "#                     \"text\": paper['pdf_text'][:2000]  # optional: limit size\n",
    "#                 }\n",
    "#             )\n",
    "#         ])\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def index_papers(papers, index, embeddings_model):\n",
    "    \"\"\"\n",
    "    Index a list of papers into Pinecone with proper text field for retrieval.\n",
    "    No namespace used — everything goes into the default (empty) namespace.\n",
    "\n",
    "    Each paper should contain at least:\n",
    "        - 'title'\n",
    "        - 'url'\n",
    "        - 'pdf_text' or 'content'\n",
    "    \"\"\"\n",
    "\n",
    "    vectors_to_upsert = []\n",
    "\n",
    "    for paper in tqdm(papers, desc=\"Indexing papers\"):\n",
    "        text = paper.get(\"pdf_text\") or paper.get(\"content\")\n",
    "        if not text:\n",
    "            print(f\"⚠️ Skipping paper '{paper.get('title', 'Unknown')}' — no text found.\")\n",
    "            continue\n",
    "\n",
    "        # Create embedding from truncated text\n",
    "        embedding = embeddings_model.embed_query(text[:3000])  # truncate to avoid token limits\n",
    "\n",
    "        # Prepare metadata\n",
    "        metadata = {\n",
    "            \"title\": paper.get(\"title\", \"Unknown\"),\n",
    "            \"url\": paper.get(\"url\", \"\"),\n",
    "            \"text\": text[:3000],   # ✅ this ensures retriever finds the 'text' key\n",
    "        }\n",
    "\n",
    "        # Each record needs a unique ID\n",
    "        paper_id = paper.get(\"id\") or paper.get(\"url\") or paper.get(\"title\", \"\")[:50]\n",
    "\n",
    "        vectors_to_upsert.append((paper_id, embedding, metadata))\n",
    "\n",
    "    # Upsert to Pinecone\n",
    "    if vectors_to_upsert:\n",
    "        index.upsert(vectors=vectors_to_upsert)\n",
    "        print(f\"✅ Indexed {len(vectors_to_upsert)} papers in Pinecone (default namespace).\")\n",
    "    else:\n",
    "        print(\"⚠️ No papers indexed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37453bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 5 papers for query 'motor engines'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing papers: 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'namespace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter research topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m papers \u001b[38;5;241m=\u001b[39m fetch_arxiv_papers(topic)\n\u001b[1;32m----> 8\u001b[0m index_papers(papers, index, embeddings_model)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Indexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(papers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in Pinecone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m, in \u001b[0;36mindex_papers\u001b[1;34m(papers, index, embeddings_model)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Upsert to Pinecone\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vectors_to_upsert:\n\u001b[1;32m---> 54\u001b[0m     index\u001b[38;5;241m.\u001b[39mupsert(vectors\u001b[38;5;241m=\u001b[39mvectors_to_upsert, namespace\u001b[38;5;241m=\u001b[39mnamespace)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Indexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vectors_to_upsert)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers into namespace \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'namespace' is not defined"
     ]
    }
   ],
   "source": [
    "# topic = input(\"Enter research topic: \")\n",
    "# papers = fetch_arxiv_papers(topic)\n",
    "# index_papers(papers)\n",
    "# print(f\"Indexed {len(papers)} papers on '{topic}' in Pinecone!\")\n",
    "\n",
    "topic = input(\"Enter research topic: \")\n",
    "papers = fetch_arxiv_papers(topic)\n",
    "index_papers(papers, index, embeddings_model)\n",
    "\n",
    "print(f\"✅ Indexed {len(papers)} papers on '{topic}' in Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b995e",
   "metadata": {},
   "source": [
    "---------- end of app first --------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad953cf",
   "metadata": {},
   "source": [
    "## App 2\n",
    " application enables users to query the indexed knowledge base by entering research questions, which the agent retrieves relevant papers for using semantic search, generates informative responses grounded in those papers, and provides proper citations with links and metadata to the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(\n",
    "    index_name=os.environ[\"PINECONE_INDEX_NAME\"],\n",
    "    embedding=embeddings_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = AzureChatOpenAI(\n",
    "#     azure_deployment=\"gpt-4.1\",\n",
    "#     temperature=0.2,\n",
    "#     max_tokens=1000\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e265f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_context(query, k=5):\n",
    "#     docs = vectorstore.similarity_search(query, k=k)\n",
    "#     context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "#     sources = [\n",
    "#         f\"- [{d.metadata.get('title', 'Untitled')}]({d.metadata.get('url', 'Unknown URL')})\"\n",
    "#         for d in docs\n",
    "#     ]\n",
    "#     return context_text, sources\n",
    "\n",
    "def retrieve_context(query, k=5):\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "\n",
    "    for d in docs:\n",
    "        # Fallback: use text from metadata if page_content is empty\n",
    "        content = d.page_content or d.metadata.get(\"text\", \"\")\n",
    "        if content:\n",
    "            context_parts.append(content)\n",
    "            sources.append(f\"- [{d.metadata.get('title', 'Untitled')}]({d.metadata.get('url', 'Unknown URL')})\")\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "    return context_text, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ac827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main research Q&A function ---\n",
    "def research_qa(query, k=5):\n",
    "    context, sources = retrieve_context(query, k=k)\n",
    "\n",
    "    # Create a structured prompt\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI research assistant. \"\n",
    "        \"Use the provided academic paper excerpts to answer the user's question clearly and concisely. \"\n",
    "        \"Cite your sources at the end in markdown link format like [Title](URL). \"\n",
    "        \"If the answer cannot be found in the papers, say so explicitly.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Question:\\n{query}\\n\\nContext:\\n{context}\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ]\n",
    "\n",
    "    # Invoke the model\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Print formatted output\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response.content)\n",
    "    print(\"\\nSources:\")\n",
    "    for s in sources:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff626b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Enter your research question: \")\n",
    "research_qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d18bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = vectorstore.similarity_search(\"sun\", k=3)\n",
    "# for i, doc in enumerate(results, 1):\n",
    "#     print(f\"\\nDoc {i}: {doc.metadata.get('title')}\")\n",
    "#     print(doc.page_content or doc.metadata.get(\"text\", \"\")[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457bea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b885c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730647e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "<!-- https://colab.research.google.com/drive/1iA6VqdRi1RPirf3PtLfPT74UJiHdPKxr?usp=sharing#scrollTo=ApA0U6w5v8er -->\n",
    "<!-- https://colab.research.google.com/drive/1-o4mnBbFtTXfaP-2FaG3X3F6q5zxRUbW?usp=sharing#scrollTo=tD4t0206sPgC -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81d960",
   "metadata": {},
   "source": [
    "to do:\n",
    "Part-2: Agentic Research Assistant\n",
    "\n",
    "Extend Part-1 by merging the two separate applications into a single unified system where an LLM-driven agent intelligently decides whether the user is in an indexing phase or a query phase based on user intent. The agent should manage state transitions explicitly—informing the user when indexing is complete and they can begin asking questions, or detecting when a user wants to start a new research topic. The system maintains conversational context and guides the user through natural transition points with clear communication about what can be done next, creating a seamless research workflow without requiring separate interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d523f09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
