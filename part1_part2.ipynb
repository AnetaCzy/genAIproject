{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78f128e",
   "metadata": {},
   "source": [
    "## Agentic Research Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d29325",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "The application enable users to conduct research on a specific topic using arXiv. It allows users to enter a research topic, which triggers an agent to search arXiv, retrieve relevant papers, extract their content, convert them to embeddings, and store them in a vector database. Once indexing is complete, the system informs the user.\n",
    "\n",
    "The second part  enables users to query the indexed knowledge base by entering research questions, which the agent retrieves relevant papers for using semantic search, generates informative responses grounded in those papers, and provides proper citations with links and metadata to the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566c9edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone: arxiv\n",
      "Retrieved 5 papers for query 'generative AI'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing papers: 100%|██████████| 5/5 [01:05<00:00, 13.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 346 chunks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `text` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " Emerging trends in generative AI include:\n",
      "\n",
      "1. **Monetization and Open Source Dynamics**: Developers are increasingly monetizing generative AI systems, even when releasing open-source versions. These open-source releases often come with restrictions or conditions to enable monetization, reflecting broader industry disputes about openness and control (e.g., the European Commission’s Google Android case) [Vincent, 2023; Dastin et al., 2023].\n",
      "\n",
      "2. **Integrated Services**: There is a notable influx of integrated generative AI services. These include search engines incorporating large language models (LLMs), personal assistants, note-taking and editing tools, creative task automation, video-editing applications, and generative AI-augmented search. This trend points to generative AI being embedded across a wide array of digital products and workflows [Reid, 2023].\n",
      "\n",
      "3. **Advanced Information Access**: Generative AI models are distinguished by their ability to generate complex, high-quality outputs based on human instructions. This enables two major new paradigms in information access:\n",
      "   - **Direct Content Generation**: AI systems can create content that directly addresses users’ information needs, providing tailored answers or products.\n",
      "   - **Innovative Synthesis**: Generative AI can recombine and synthesize existing information in novel ways, producing new, coherent outputs that go beyond simple retrieval [arXiv:2501.02842v1].\n",
      "\n",
      "4. **Expansion of Information Retrieval (IR)**: The future of IR research is shifting towards using generative AI to handle more complex information tasks and develop general system architectures. Instead of merely retrieving documents, these systems will perform sophisticated information processing and planning, potentially replacing traditional human roles unless humans leverage AI to extend their capabilities [Yao et al., 153].\n",
      "\n",
      "In summary, generative AI is trending towards monetization, integration into diverse applications, advanced content generation and synthesis, and a transformation of information retrieval systems to handle more complex tasks.\n",
      "\n",
      "**Sources:**\n",
      "- [arXiv:2501.02842v1](https://arxiv.org/abs/2501.02842)\n",
      "- Vincent, 2023; Dastin et al., 2023; Reid, 2023 (as cited in provided context)\n",
      "\n",
      "Sources:\n",
      "- [AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI]( http://arxiv.org/abs/2308.02033v1 )\n",
      "- [Foundations of GenIR]( http://arxiv.org/abs/2501.02842v1 )\n",
      "- [Foundations of GenIR]( http://arxiv.org/abs/2501.02842v1 )\n",
      "- [Foundations of GenIR]( http://arxiv.org/abs/2501.02842v1 )\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import arxiv\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain + OpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Pinecone v4 + LangChain Pinecone VectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"arxiv\")\n",
    "\n",
    "# Azure OpenAI LLM + Embeddings\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4.1\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "embeddings_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "#  Pinecone v4 Initialization\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create Index if doesn't exist\n",
    "if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "print(\"Connected to Pinecone:\", PINECONE_INDEX_NAME)\n",
    "\n",
    "# Fetch and parse arXiv PDF data\n",
    "def fetch_arxiv_papers(query, max_results=5):\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    papers = []\n",
    "    for result in client.results(search):\n",
    "        try:\n",
    "            pdf_url = result.pdf_url\n",
    "            response = requests.get(pdf_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            pdf_file = BytesIO(response.content)\n",
    "            reader = PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "\n",
    "            papers.append({\n",
    "                \"id\": result.entry_id,\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"url\": result.entry_id,\n",
    "                \"pdf_text\": text,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {result.title[:50]} due to error: {e}\")\n",
    "\n",
    "    print(f\"Retrieved {len(papers)} papers for query '{query}'\")\n",
    "    return papers\n",
    "\n",
    "# Chunk + Embed + Upsert into Pinecone\n",
    "def index_papers(papers):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    for paper in tqdm(papers, desc=\"Indexing papers\"):\n",
    "        text = paper.get(\"pdf_text\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        docs = splitter.split_documents([\n",
    "            Document(page_content=text, metadata=paper)\n",
    "        ])\n",
    "\n",
    "        for idx, d in enumerate(docs):\n",
    "            embedding = embeddings_model.embed_query(d.page_content)\n",
    "            chunk_id = f\"{paper['id']}_chunk{idx}\"\n",
    "\n",
    "            metadata = {\n",
    "                \"title\": paper[\"title\"],\n",
    "                \"url\": paper[\"url\"],\n",
    "                \"text\": d.page_content,\n",
    "                \"chunk_id\": idx\n",
    "            }\n",
    "\n",
    "            vectors.append((chunk_id, embedding, metadata))\n",
    "\n",
    "    if vectors:\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Indexed {len(vectors)} chunks!\")\n",
    "    else:\n",
    "        print(\"No data to upsert!\")\n",
    "        \n",
    "### Part 2\n",
    "# VectorStore for Retrieval\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=PINECONE_INDEX_NAME,\n",
    "    embedding=embeddings_model\n",
    ")\n",
    "\n",
    "# RAG Query System\n",
    "def retrieve_context(query, k=5):\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    sources = [f\"- [{d.metadata.get('title')}]( {d.metadata.get('url')} )\" for d in docs]\n",
    "    return context, sources\n",
    "\n",
    "def research_qa(query):\n",
    "    context, sources = retrieve_context(query)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI research assistant. \"\n",
    "        \"Answer based ONLY on the provided excerpts from papers. \"\n",
    "        \"Cite your sources at the end using markdown links. \"\n",
    "        \"If the context lacks an answer, say so clearly.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"Question:\\n{query}\\n\\nContext:\\n{context}\")\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"\\nAnswer:\\n\", response.content)\n",
    "    print(\"\\nSources:\")\n",
    "    for s in sources:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "topic = input(\"Enter research topic: \")\n",
    "papers = fetch_arxiv_papers(topic)\n",
    "index_papers(papers)\n",
    "\n",
    "question = input(\"\\nEnter your research question: \")\n",
    "research_qa(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2556ee4",
   "metadata": {},
   "source": [
    "### PART 2\n",
    "Part-2: Agentic Research Assistant\n",
    "\n",
    "Extend of Part-1 by merging the two separate applications into a single unified system where an LLM-driven agent intelligently decides whether the user is in an indexing phase or a query phase based on user intent. The agent should manage state transitions explicitly informing the user when indexing is complete and they can begin asking questions, or detecting when a user wants to start a new research topic. The system maintains conversational context and guides the user through natural transition points with clear communication about what can be done next, creating a seamless research workflow without requiring separate interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e623ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! Enter a research topic to fetch papers.\n",
      "Type 'reset' for a new topic, 'quit' to exit.\n",
      "\n",
      " Fetching 3 papers for topic: 'machine learning'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PDFs: 100%|██████████| 3/3 [00:13<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating embeddings and preparing for indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 3/3 [02:14<00:00, 44.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Uploading chunks to Pinecone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing chunks: 100%|██████████| 15/15 [00:30<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Indexed 704 chunks from 3 papers.\n",
      "\n",
      " Indexed papers for 'machine learning'. You can now ask questions or type a new topic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Your question: What are emerging trends in machine learning?\n",
      "\n",
      "\n",
      "Answer:\n",
      " Emerging trends in machine learning include:\n",
      "\n",
      "1. **Analysis of Massive and Complex Datasets**: The increasing availability of open and large data sources has enabled machine learning to analyze massive datasets, identify complex patterns and relationships, and perform non-linear forecasting, which traditional survey methods may not capture [1,2].\n",
      "\n",
      "2. **Applications in Diverse Fields**: Machine learning is being applied in areas such as autonomous driving, protein structure prediction, and medicine, demonstrating its transformative impact across disciplines [He et al., 2019; Grigorescu et al., 2020; Jumper].\n",
      "\n",
      "3. **Recurring Concept Drift in Data Streams**: There is growing interest in handling recurring concept drift in data streams, which refers to changes in data distributions over time. This is important for applications where data evolves, requiring models to adapt continuously [Suárez-Cetrulo et al., 2023].\n",
      "\n",
      "4. **Active Learning and Feature Reconstruction**: New strategies such as active learning based on feature reconstruction are being developed to improve model performance, especially in scenarios where labeled data is scarce [Tang et al., 2018].\n",
      "\n",
      "5. **Challenges and Focus Areas**: Key challenges driving research include data quality, privacy, security, explainability, accuracy, reproducibility, timeliness, and cost effectiveness. Addressing these challenges is essential for broader adoption and trust in machine learning systems [arXiv:2306.04338v1].\n",
      "\n",
      "6. **Model Interpretability**: There is a trend towards making machine learning models more interpretable, which is crucial for understanding model decisions and for applications in sensitive domains [Guyon & Elisseeff, 2003].\n",
      "\n",
      "Sources:\n",
      "- arXiv:2306.04338v1 [stat.ML]\n",
      "- Suárez-Cetrulo AL, Quintana D, Cervantes A (2023)\n",
      "- Tang Q, Li D, Xi Y (2018)\n",
      "- He, J. et al. (2019)\n",
      "- Guyon, I. & Elisseeff, A. (2003)\n",
      "\n",
      "Sources:\n",
      "- [Changing Data Sources in the Age of Machine Learning for Official Statistics](http://arxiv.org/abs/2306.04338v1) (chunk 8.0)\n",
      "- [Changing Data Sources in the Age of Machine Learning for Official Statistics](http://arxiv.org/abs/2306.04338v1) (chunk 6.0)\n",
      "- [DOME: Recommendations for supervised machine learning validation in biology](http://arxiv.org/abs/2006.16189v4) (chunk 118.0)\n",
      "- [Active learning for data streams: a survey](http://arxiv.org/abs/2302.08893v4) (chunk 425.0)\n",
      "- [Physics-Inspired Interpretability Of Machine Learning Models](http://arxiv.org/abs/2304.02381v2) (chunk 3.0)\n",
      "Goodbye! \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import arxiv\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# env & clients\n",
    "load_dotenv()\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"arxiv\")\n",
    "\n",
    "llm = AzureChatOpenAI(azure_deployment=\"gpt-4.1\", temperature=0.2, max_tokens=1000)\n",
    "embeddings_model = AzureOpenAIEmbeddings(azure_deployment=\"text-embedding-3-small\")\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(name=PINECONE_INDEX_NAME, dimension=1536, metric=\"cosine\",\n",
    "                    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "vectorstore = PineconeVectorStore.from_existing_index(index_name=PINECONE_INDEX_NAME, embedding=embeddings_model)\n",
    "\n",
    "\n",
    "# parameters to avoid Pinecone limits\n",
    "MAX_CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "MAX_PINECONE_METADATA_SIZE = 3_500_000  # bytes (3.5MB, safe below 4MB limit)\n",
    "MAX_METADATA_TEXT = 500  # truncate metadata text to 500 chars (safe under 40KB)\n",
    "\n",
    "def split_large_chunk(text, initial_chunk_size=MAX_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=initial_chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# fetching and indexing data\n",
    "def fetch_and_index(topic, max_results=3):\n",
    "    search = arxiv.Search(query=topic, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "\n",
    "    print(f\"\\n Fetching {max_results} papers for topic: '{topic}'\")\n",
    "    results = list(client.results(search))\n",
    "    for r in tqdm(results, desc=\"Downloading PDFs\"):\n",
    "        try:\n",
    "            pdf = requests.get(r.pdf_url, timeout=20)\n",
    "            pdf.raise_for_status()\n",
    "            reader = PdfReader(BytesIO(pdf.content))\n",
    "            text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            papers.append({\"id\": r.entry_id, \"title\": r.title, \"url\": r.entry_id, \"pdf_text\": text})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    vectors = []\n",
    "    print(\"\\n Creating embeddings and preparing for indexing...\")\n",
    "\n",
    "    for paper in tqdm(papers, desc=\"Processing papers\"):\n",
    "        base_chunks = split_large_chunk(paper[\"pdf_text\"], initial_chunk_size=MAX_CHUNK_SIZE)\n",
    "        for idx, chunk in enumerate(base_chunks):\n",
    "            embedding = embeddings_model.embed_query(chunk)\n",
    "            metadata = {\n",
    "                \"title\": paper[\"title\"],\n",
    "                \"url\": paper[\"url\"],\n",
    "                \"chunk_id\": idx,\n",
    "                \"text\": chunk\n",
    "            }\n",
    "            vectors.append((f\"{paper['id']}_chunk{idx}\", embedding, metadata))\n",
    "\n",
    "    print(\"\\n Uploading chunks to Pinecone...\")\n",
    "    for i in tqdm(range(0, len(vectors), BATCH_SIZE), desc=\"Indexing chunks\"):\n",
    "        batch = vectors[i:i+BATCH_SIZE]\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "    print(f\"\\n Indexed {len(vectors)} chunks from {len(papers)} papers.\")\n",
    "    return papers\n",
    "\n",
    "# retrive and answer questions\n",
    "def answer_question(query):\n",
    "    print(f\"\\n Your question: {query}\\n\")\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    sources = [f\"- [{d.metadata.get('title')}]({d.metadata.get('url')}) (chunk {d.metadata.get('chunk_id',0)})\" for d in docs]\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI research assistant. Answer ONLY using the excerpts provided. \"\n",
    "        \"Cite sources at the end. If the context doesn't answer, say so clearly.\"\n",
    "    )\n",
    "\n",
    "    messages = [SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=f\"Question:\\n{query}\\n\\nContext:\\n{context}\")]\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"\\nAnswer:\\n\", response.content)\n",
    "    print(\"\\nSources:\")\n",
    "    for s in sources:\n",
    "        print(s)\n",
    "\n",
    "# intent classifier to decide indexing vs querying\n",
    "def classify_intent(user_input, current_topic):\n",
    "    \"\"\"\n",
    "    Decide if the user wants to 'index' a new topic or 'query' the current topic.\n",
    "    \"\"\"\n",
    "    if not current_topic:\n",
    "        return \"index\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"Current topic: {current_topic}\\n\"\n",
    "        f\"User input: \\\"{user_input}\\\"\\n\"\n",
    "        \"Decide the intent: 'index' if user wants a new topic, 'query' if asking a question. \"\n",
    "        \"Respond with only 'index' or 'query'.\"\n",
    "    )\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    intent = result.content.strip().lower().replace(\".\", \"\")\n",
    "    return \"index\" if intent == \"index\" else \"query\"\n",
    "\n",
    "# main loop\n",
    "print(\" Hello! Enter a research topic to fetch papers.\")\n",
    "print(\"Type 'reset' for a new topic, 'quit' to exit.\")\n",
    "\n",
    "current_topic = None\n",
    "indexed = False\n",
    "\n",
    "while True:\n",
    "    # Dynamic instructions for users to decide next action\n",
    "    if not current_topic:\n",
    "        prompt_text = \"Type a research topic to fetch papers (or 'quit' to exit): \"\n",
    "    elif indexed:\n",
    "        prompt_text = \"Ask a question or type a new topic to fetch papers: \"\n",
    "    else:\n",
    "        prompt_text = \"Type a research topic to fetch papers (or 'quit' to exit): \"\n",
    "\n",
    "    user_input = input(f\"\\nYou: {prompt_text}\").strip()\n",
    "\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Goodbye! \")\n",
    "        break\n",
    "\n",
    "    # Model detects if this is a new topic or a query\n",
    "    intent = classify_intent(user_input, current_topic)\n",
    "\n",
    "    if intent == \"index\":\n",
    "        # User wants to index a new topic\n",
    "        current_topic = user_input\n",
    "        fetch_and_index(current_topic)\n",
    "        indexed = True\n",
    "        print(f\"\\n Indexed papers for '{current_topic}'. You can now ask questions or type a new topic.\")\n",
    "    else:\n",
    "        # User is asking a question\n",
    "        if not indexed:\n",
    "            print(\"\\n No research indexed yet! Please provide a topic first.\")\n",
    "            continue\n",
    "        answer_question(user_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813c0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
